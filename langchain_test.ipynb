{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dotenv is a library to interact with .env files\n",
    "load_dotenv reads the .env file and loads the data into the working environment\n",
    "\n",
    "find_dotenv searches for the nearest .env file in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LLM Wrappers\n",
    "\n",
    "\"note: temperature determines how random or deterministic the LLM's response is - \n",
    "lower temperature means it picks most likely next word every time\n",
    "higher temperature -> higher sample size of words -> res more creative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name = \"gpt-4o-mini\")\n",
    "res = llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: Payment at openai, doesn't work yet\n",
    "\n",
    "Ganz genau âœ… â€” jedes Mal wenn du ein Chatmodell (z. B. ChatOpenAI) aufrufst, wird die Antwort automatisch in ein AIMessage-Objekt gepackt.\n",
    "Das ist der Standard-Container, in dem LangChain Chat-Antworten speichert.\n",
    "Basically Daten des Responses\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content=\"Katzen kÃ¶nnen Ã¼ber 100 verschiedene Laute von sich geben, wÃ¤hrend Hunde nur etwa 10 haben!\",\n",
    "    additional_kwargs={},\n",
    "    response_metadata={\n",
    "        'token_usage': {\n",
    "            'completion_tokens': 24,\n",
    "            'prompt_tokens': 15,\n",
    "            'total_tokens': 39\n",
    "        },\n",
    "        'model_name': 'gpt-4o-mini'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage, # response from AI\n",
    "    HumanMessage, # message from user\n",
    "    SystemMessage # how the AI should behave - contextual prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = OpenAI(model_name = \"gpt-4o-mini\", temperature=0.3)\n",
    "messages = [ # input messages\n",
    "    SystemMessage(content=\"You are an expert scientist\"),\n",
    "    HumanMessage(content=\"Write a python script that trains a neural network on simulated data.\")\n",
    "]\n",
    "response = chat(messages) # send to the model # get response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Promp Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept of {concept} in a couple of lines.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template, # updated template variable with concept input\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(prompt.format(concept=\"regularization\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chains\n",
    "\n",
    "A chain in LangChain is a pipeline that links prompts, LLMs, and other components (like retrievers or output parsers) together into a single callable workflow.\n",
    "\n",
    "In this case it's kinda like a composite function f(g(x)):\n",
    "\n",
    "- the outer function is the LLM \n",
    "- the inner function is the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt) # create chain\n",
    "\n",
    "# Run chain only specifying the concept\n",
    "print(chain.run(\"overfitting\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we can make a sequential model\n",
    "\n",
    "output of chain 1 = input of chain 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words.\"\n",
    ")\n",
    "chain2 = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Sequential Chain returns both the output of the first description of the concept AND the second description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "overall_chain = simpleSequentialChain(chains=[chain, chain2], verbose=True)\n",
    "\n",
    "# Run the overall chain, only specifying the concept of chain 1\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Embeddings & Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split text into chunks - Chunking\n",
    "\n",
    "This is how a document looks like:\n",
    "- the actual content\n",
    "- metadata -> dictionary of tags about the data (similar to a json file)\n",
    "\n",
    "```python\n",
    "from langchain.schema import Document\n",
    "\n",
    "doc = Document(\n",
    "    page_content=\"Paris is the capital of France.\",\n",
    "    metadata={\"source\": \"wikipedia\", \"category\": \"geography\"}\n",
    ")\n",
    "```\n",
    "\n",
    "default seperators for RecursiveCharacterTextSplitter:\n",
    "[\"\\n\\n\", \"\\n\", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, # maximum size of each chunk\n",
    "    chunk_overlap=0, # how many characters are repeated between chunks (so context flows across boundaries)\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation]) # texts is an array of Document objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Turn into an embedding (a vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query_result is just printing to test and debug the first embeddings chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(texts[0].page_content) # get embedding for the first chunk\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import pinecone â†’ official Pinecone client â†’ lets you connect to Pinecone and create/manage indexes.\n",
    "- from langchain.vectorstores import Pinecone â†’ LangChain wrapper â†’ lets LangChain talk to Pinecone easily (add texts, search, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone # vector database\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "#init\n",
    "pinecone.init(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\"), \n",
    "    environment=os.getenv(\"PINECONE_ENVIRONMENT_REGION\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. texts â†’ a list of Document objects (chunks of text you split earlier).\n",
    "2. embeddings â†’ your embedding model (e.g. OpenAIEmbeddings() or HuggingFaceEmbeddings()).\n",
    "3. index_name=\"langchain-quickstart\" â†’ the name of the Pinecone index you already created.\n",
    "4. ðŸ‘‰ Pinecone.from_documents(...) does 3 things at once:\n",
    "- Creates embeddings for each document.\n",
    "- Stores them in Pinecone under that index.\n",
    "- Returns a LangChain VectorStore object (search) that you can now query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain-quickstart\" # basically the name for this vectore storage space\n",
    "\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name) # create vector store under this index name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search based on prompts\n",
    "1. Query â†’ turned into an embedding vector.\n",
    "2. Vector â†’ sent to Pinecone index.\n",
    "3. Pinecone â†’ finds nearest stored vectors (similar chunks).\n",
    "4. LangChain â†’ wraps them back as Documents with text + metadata.\n",
    "5. ðŸ‘‰ result = list of the most relevant chunks.\n",
    "\n",
    "\n",
    "Pinecone doesnâ€™t generate answers â€” it just stores your document embeddings and returns the most relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is magical about autoencoders?\"\n",
    "\n",
    "result = search.similarity_search(query) # search for similar chunks to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tryout AI Agent tools from langchain\n",
    "\n",
    "##### REPL = Readâ€“Evalâ€“Print Loop \n",
    "Itâ€™s an interactive programming shell that:\n",
    "1. Reads your input (e.g. 2 + 2)\n",
    "2. Evaluates it (runs the code)\n",
    "3. Prints the result (4)\n",
    "4. Loops back for the next command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent creation\n",
    "from langchain_experimental.agents.agent_toolkits import create_python_agent # updated location\n",
    "\n",
    "\n",
    "# Python REPL tool + utility\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "***REMOVED*** LLM wrapper\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent_executor can now run python code as per REPL protocol\n",
    "\n",
    "\n",
    "Note: in LangChain (and in Python generally), verbose=True just means:\n",
    "\n",
    "ðŸ‘‰ print extra details about whatâ€™s happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_python_agent (\n",
    "    llm=OpenAI(temperature=0, max_tokens=1000, model_name=\"gpt-4o-mini\"), \n",
    "    tool=PythonREPLTool(PythonREPL()), \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then i can give it a prompt and it'll solve it for me - e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"Find the roots (zeros) of the quadratic function f(x) = 2x^2 - 4x - 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it'll create a python script, agent_executor REPL allows the llm to run the script and return the result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragplus_sm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
